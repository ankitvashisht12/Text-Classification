{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords , wordnet\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting folder names for classes in dataset\n",
    "\n",
    "Y = []                            \n",
    "\n",
    "for folders in os.listdir('./20_newsgroups/'):\n",
    "    Y.append(folders)\n",
    "    \n",
    "y_classes = [i for i in range(len(Y))]\n",
    "y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'talk.politics.guns': 0,\n",
       " 'rec.sport.baseball': 1,\n",
       " 'talk.politics.misc': 2,\n",
       " 'comp.graphics': 3,\n",
       " 'sci.electronics': 4,\n",
       " 'soc.religion.christian': 5,\n",
       " 'comp.sys.ibm.pc.hardware': 6,\n",
       " 'rec.motorcycles': 7,\n",
       " 'comp.sys.mac.hardware': 8,\n",
       " 'alt.atheism': 9,\n",
       " 'sci.crypt': 10,\n",
       " 'misc.forsale': 11,\n",
       " 'sci.med': 12,\n",
       " 'sci.space': 13,\n",
       " 'rec.sport.hockey': 14,\n",
       " 'talk.religion.misc': 15,\n",
       " 'comp.os.ms-windows.misc': 16,\n",
       " 'comp.windows.x': 17,\n",
       " 'rec.autos': 18,\n",
       " 'talk.politics.mideast': 19}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary_y\n",
    "\n",
    "dict_Y = {}\n",
    "for i in range(len(Y)):\n",
    "    dict_Y[Y[i]] = i\n",
    "    \n",
    "dict_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stop = stopwords.words('english')\n",
    "stop += punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(word):\n",
    "    tag = pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADJ\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train & Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Warning : This cell will take time to run. So please wait.\n",
    "'''\n",
    "\n",
    "documents = []\n",
    "\n",
    "for dir_path , dir_name, file_name in os.walk('./20_newsgroups/'):\n",
    "    if len(file_name) == 0:\n",
    "        Y_labels = dir_name\n",
    "    else:\n",
    "        temp = dir_path[16:]\n",
    "        #wordlist = []\n",
    "        for files in file_name:\n",
    "            lines = ''\n",
    "            with open(dir_path+'/'+files, 'r' ,encoding = \"ISO-8859-1\") as f:\n",
    "                for line in f:\n",
    "                    for word in word_tokenize(line):\n",
    "                        if word.lower() not in stop and word.isalpha():\n",
    "                            word = lemmatizer.lemmatize(word, pos = get_pos_tag(word))\n",
    "                            lines += word\n",
    "                            lines += ' '\n",
    "                            #wordlist.append(word)\n",
    "            documents.append((lines, dict_Y[temp]))                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [ y for doc, y in documents]\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [ doc for doc, y in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### X_test & Y_test\n",
    "\n",
    "\n",
    "\n",
    "- NOTE :  I have used '20_newsgroups' as training dataset and 'mini_newsgroups' as testing dataset (As instructed by TA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Warning : This cell will take time to run. So please wait.\n",
    "'''\n",
    "\n",
    "X_test  = []\n",
    "\n",
    "for dir_path , dir_name, file_name in os.walk('./mini_newsgroups/'):\n",
    "    if len(file_name) == 0:\n",
    "        Y_test_labels = dir_name\n",
    "    else:\n",
    "        temp = dir_path[18:]\n",
    "        #wordlist = []\n",
    "        for files in file_name:\n",
    "            lines_test = ''\n",
    "            with open(dir_path+'/'+files, 'r' ,encoding = \"ISO-8859-1\") as f:\n",
    "                for line in f:\n",
    "                    for word in word_tokenize(line):\n",
    "                        if word.lower() not in stop and word.isalpha():\n",
    "                            word = lemmatizer.lemmatize(word, pos = get_pos_tag(word))\n",
    "                            lines_test +=  word\n",
    "                            lines_test += ' '\n",
    "                            #wordlist.append(word)\n",
    "            X_test.append((lines_test, dict_Y[temp]))  \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = [ y for doc, y in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = [ doc for doc ,y in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of features \n",
    "\n",
    "k = 3000                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = TfidfVectorizer(max_features = k , max_df=0.8)\n",
    "X_train = count_vec.fit_transform(docs)\n",
    "X_train = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vec.transform(docs_test)\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visuliazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2990</th>\n",
       "      <th>2991</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7         8     9     ...  2990  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000   0.0  ...   0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000   0.0  ...   0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000   0.0  ...   0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.074351   0.0  ...   0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000   0.0  ...   0.0   \n",
       "\n",
       "   2991  2992  2993      2994  2995  2996  2997  2998  2999  \n",
       "0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0  0.184983   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(X_train)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2990</th>\n",
       "      <th>2991</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.017938</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.016201</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.020664</td>\n",
       "      <td>0.030185</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.012310</td>\n",
       "      <td>0.015382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>0.021691</td>\n",
       "      <td>0.013410</td>\n",
       "      <td>0.019341</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.020568</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.010732</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>0.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.872059</td>\n",
       "      <td>0.547026</td>\n",
       "      <td>0.844920</td>\n",
       "      <td>0.391783</td>\n",
       "      <td>0.345596</td>\n",
       "      <td>0.712626</td>\n",
       "      <td>0.737732</td>\n",
       "      <td>0.244250</td>\n",
       "      <td>0.491812</td>\n",
       "      <td>0.756248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718446</td>\n",
       "      <td>0.749224</td>\n",
       "      <td>0.347986</td>\n",
       "      <td>0.274223</td>\n",
       "      <td>0.422118</td>\n",
       "      <td>0.625549</td>\n",
       "      <td>0.600091</td>\n",
       "      <td>0.273232</td>\n",
       "      <td>0.616453</td>\n",
       "      <td>0.306615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4     \\\n",
       "count  19997.000000  19997.000000  19997.000000  19997.000000  19997.000000   \n",
       "mean       0.001157      0.001025      0.000747      0.001492      0.004868   \n",
       "std        0.017938      0.015184      0.016201      0.012560      0.020664   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.872059      0.547026      0.844920      0.391783      0.345596   \n",
       "\n",
       "               5             6             7             8             9     \\\n",
       "count  19997.000000  19997.000000  19997.000000  19997.000000  19997.000000   \n",
       "mean       0.002379      0.001310      0.001447      0.001073      0.000883   \n",
       "std        0.030185      0.017668      0.012104      0.012310      0.015382   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.712626      0.737732      0.244250      0.491812      0.756248   \n",
       "\n",
       "       ...          2990          2991          2992          2993  \\\n",
       "count  ...  19997.000000  19997.000000  19997.000000  19997.000000   \n",
       "mean   ...      0.000807      0.004967      0.001392      0.004603   \n",
       "std    ...      0.017983      0.021691      0.013410      0.019341   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      0.718446      0.749224      0.347986      0.274223   \n",
       "\n",
       "               2994          2995          2996          2997          2998  \\\n",
       "count  19997.000000  19997.000000  19997.000000  19997.000000  19997.000000   \n",
       "mean       0.002874      0.002584      0.000957      0.000926      0.001050   \n",
       "std        0.020482      0.020568      0.015526      0.010732      0.015403   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.422118      0.625549      0.600091      0.273232      0.616453   \n",
       "\n",
       "               2999  \n",
       "count  19997.000000  \n",
       "mean       0.000432  \n",
       "std        0.006368  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        0.306615  \n",
       "\n",
       "[8 rows x 3000 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB Implementation\n",
    "    - Inbuilt Multinomial Naive Bayes implementation from SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()            # Multinomial Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)    # fitting the training data\n",
    "ypred = clf.predict(X_test)  # predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  7, 18, ...,  5, 17,  9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred                          # predicted result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score, Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.841\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score :',accuracy_score(Y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :-\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.88      0.79       100\n",
      "           1       0.97      0.94      0.95       100\n",
      "           2       0.75      0.62      0.68       100\n",
      "           3       0.72      0.83      0.77       100\n",
      "           4       0.84      0.81      0.83       100\n",
      "           5       0.88      0.98      0.93       100\n",
      "           6       0.75      0.76      0.76       100\n",
      "           7       0.92      0.90      0.91       100\n",
      "           8       0.84      0.78      0.81       100\n",
      "           9       0.73      0.86      0.79       100\n",
      "          10       0.97      0.92      0.94       100\n",
      "          11       0.84      0.88      0.86       100\n",
      "          12       0.97      0.91      0.94       100\n",
      "          13       0.92      0.93      0.93       100\n",
      "          14       0.94      0.96      0.95       100\n",
      "          15       0.73      0.43      0.54       100\n",
      "          16       0.72      0.79      0.75       100\n",
      "          17       0.81      0.86      0.83       100\n",
      "          18       0.90      0.86      0.88       100\n",
      "          19       0.96      0.92      0.94       100\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "Confusion Matrix :-\n",
      "\n",
      " [[88  0  5  0  0  0  0  2  0  0  1  0  0  0  0  3  0  1  0  0]\n",
      " [ 0 94  0  1  0  0  0  0  1  0  0  0  0  0  3  0  0  0  1  0]\n",
      " [20  1 62  0  0  0  0  0  0  0  1  0  1  2  2  6  0  0  1  4]\n",
      " [ 0  0  0 83  1  0  3  0  0  0  1  1  0  0  0  0  6  5  0  0]\n",
      " [ 0  0  0  3 81  0  4  0  5  0  0  1  0  1  0  0  4  0  1  0]\n",
      " [ 1  0  0  1  0 98  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  3  0 76  0  6  0  0  4  0  0  0  0  9  0  0  0]\n",
      " [ 0  0  1  1  0  0  0 90  0  0  0  2  0  1  0  0  0  2  3  0]\n",
      " [ 0  0  0  3  2  0  7  0 78  0  0  2  1  0  0  0  5  1  1  0]\n",
      " [ 0  0  1  0  0  3  0  1  0 86  0  0  1  0  0  7  0  0  1  0]\n",
      " [ 0  0  2  1  1  0  1  0  1  0 92  1  0  0  0  0  0  0  1  0]\n",
      " [ 1  0  0  1  3  0  1  1  2  0  0 88  0  0  0  0  2  0  1  0]\n",
      " [ 0  1  0  3  0  0  0  1  0  3  0  0 91  0  0  0  0  1  0  0]\n",
      " [ 1  0  1  1  0  0  0  0  0  0  0  0  0 93  0  0  0  4  0  0]\n",
      " [ 0  1  1  1  0  0  0  1  0  0  0  0  0  0 96  0  0  0  0  0]\n",
      " [12  0  6  2  0  8  0  0  0 29  0  0  0  0  0 43  0  0  0  0]\n",
      " [ 0  0  0  6  1  0  7  0  0  0  0  1  0  1  0  0 79  5  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0  0  1  0  1  1  0  4 86  0  0]\n",
      " [ 1  0  0  0  4  0  1  1  0  0  0  3  0  2  0  0  1  1 86  0]\n",
      " [ 0  0  4  0  0  2  0  1  0  0  0  1  0  0  0  0  0  0  0 92]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report :-\\n\\n', classification_report(Y_test, ypred))\n",
    "print('Confusion Matrix :-\\n\\n', confusion_matrix(Y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB Implementation ( Self )\n",
    "- Self Multinomial Naive Bayes Implenetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    - MultinomialNaiveBayes Class\n",
    "'''\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    # __init__ function \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # fit function :- takes two arguments : Xtrain dataset and ytrain dataset and fits the data and returns the \n",
    "    # dictinary 'count' that contain probability of each word in each document that are part of vocublary.\n",
    "    def fit(self , xtrain , ytrain):\n",
    "        self.count = {}\n",
    "        classes = set(ytrain)                    # unique classes in Ytrain \n",
    "        for c in classes:           \n",
    "            self.count[c] = {}                       # adding every unique class in count dictionary as key\n",
    "            self.count['total_data'] = len(ytrain)       # adding total_data key and length of ytrain as its value\n",
    "            current_class_row = (ytrain == c)            \n",
    "            xtrain_current = xtrain[current_class_row]   # only those rows of xtrain that has class c\n",
    "            ytrain_current = ytrain[current_class_row]   # only those points of ytrain that has value c\n",
    "            self.count[c]['total_count'] = (current_class_row).sum()  # total_count = total no. of class c in Ytrain\n",
    "\n",
    "            for j in range(len(features)):   # instead of len(features) do xtrain.shape[1]\n",
    "                 self.count[c][j] = xtrain_current[:,j].sum()             \n",
    "            \n",
    "            self.count[c]['total_feature_count'] = np.sum(xtrain_current)\n",
    "    \n",
    "    # _probability fn :\n",
    "    #                 - takes 2 argument : x = testing data point (row of testing dataset)\n",
    "    #                                    : cur_class = current class ( From Y_train )\n",
    "    #                 - Returns output i.e. sum of log probability of all the words that testing data point contains.\n",
    "    def _probability(self, x, cur_class):\n",
    "        output = np.log(self.count[cur_class]['total_count']) -  np.log(self.count['total_data'])\n",
    "        num_features = len(self.count[cur_class].keys())-2\n",
    "        for j in range(num_features):\n",
    "            if(x[j] == 0):\n",
    "                cur_xj_prob = 0\n",
    "            else:\n",
    "                count_cur_class_with_value_xj = self.count[cur_class][j] +1\n",
    "                count_cur_class = self.count[cur_class]['total_feature_count'] + len(self.count[cur_class].keys()) -2\n",
    "                cur_xj_prob = np.log(count_cur_class_with_value_xj )- np.log(count_cur_class)\n",
    "            output += cur_xj_prob\n",
    "\n",
    "        return output  \n",
    "    \n",
    "    # _predictSinglePoint fn : takes only one argument i.e.  x = testing data point (row of testing dataset)\n",
    "    #                       Returns the best class (best_c) based on the probability it gets from _probability(x, cur_class)\n",
    "    def _predictSinglePoint(self, x):\n",
    "        classes = self.count.keys()\n",
    "        best_p = -10**11\n",
    "        best_c = -1\n",
    "        for cur_class in classes:\n",
    "            if cur_class == 'total_data':\n",
    "                continue\n",
    "            p = self._probability(x, cur_class)\n",
    "\n",
    "            if p > best_p:\n",
    "                best_p = p\n",
    "                best_c = cur_class\n",
    "        return best_c\n",
    "    \n",
    "    # predict fn : takes 1 argument i.e. whole Xtest dataset\n",
    "    #              returns list of prediction (y_pred)\n",
    "    def predict(self, x_test):\n",
    "        self.y_pred = []\n",
    "        for x in x_test:\n",
    "            x_class = self._predictSinglePoint(x)\n",
    "            self.y_pred.append(x_class)\n",
    "        return self.y_pred\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_self = MultinomialNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_self.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_cls = clf_self.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score , Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.8415\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score :',accuracy_score(Y_test, ypred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :-\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78       100\n",
      "           1       0.95      0.94      0.94       100\n",
      "           2       0.72      0.62      0.67       100\n",
      "           3       0.66      0.81      0.73       100\n",
      "           4       0.80      0.83      0.81       100\n",
      "           5       0.93      0.98      0.96       100\n",
      "           6       0.83      0.75      0.79       100\n",
      "           7       0.88      0.90      0.89       100\n",
      "           8       0.85      0.80      0.82       100\n",
      "           9       0.72      0.87      0.79       100\n",
      "          10       0.96      0.91      0.93       100\n",
      "          11       0.88      0.88      0.88       100\n",
      "          12       0.96      0.91      0.93       100\n",
      "          13       0.94      0.92      0.93       100\n",
      "          14       0.98      0.94      0.96       100\n",
      "          15       0.68      0.43      0.53       100\n",
      "          16       0.78      0.83      0.81       100\n",
      "          17       0.83      0.84      0.84       100\n",
      "          18       0.84      0.89      0.86       100\n",
      "          19       0.97      0.91      0.94       100\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "Confusion Matrix :-\n",
      "\n",
      " [[87  0  6  0  0  0  0  1  0  1  1  0  0  0  0  3  0  1  0  0]\n",
      " [ 0 94  0  1  0  0  0  1  1  1  0  0  1  0  1  0  0  0  0  0]\n",
      " [20  1 62  0  0  0  0  0  0  0  2  0  1  2  1  8  0  0  0  3]\n",
      " [ 0  0  0 81  2  0  3  0  1  0  0  0  0  0  0  1  6  5  1  0]\n",
      " [ 0  0  0  3 83  0  1  1  2  0  0  2  1  0  0  0  2  0  5  0]\n",
      " [ 1  0  0  1  0 98  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  4  0 75  0  7  0  0  3  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  2  0  0  0 90  0  0  0  1  0  1  0  0  0  2  4  0]\n",
      " [ 0  0  0  3  2  0  6  0 80  0  0  1  0  2  0  0  4  1  1  0]\n",
      " [ 0  0  2  0  0  1  0  1  0 87  0  0  0  0  0  8  0  0  1  0]\n",
      " [ 1  0  0  1  3  0  0  0  1  0 91  1  0  0  0  0  1  0  1  0]\n",
      " [ 0  0  0  2  3  0  0  2  2  0  0 88  0  0  0  0  1  0  2  0]\n",
      " [ 0  0  2  3  0  0  0  1  0  2  0  0 91  0  0  0  0  1  0  0]\n",
      " [ 0  0  1  1  1  0  0  0  0  0  0  0  0 92  0  0  0  3  2  0]\n",
      " [ 0  3  1  1  0  0  0  1  0  0  0  0  0  0 94  0  0  0  0  0]\n",
      " [11  0  7  2  0  5  0  0  0 30  0  0  1  0  0 43  1  0  0  0]\n",
      " [ 0  1  0  6  1  0  5  0  0  0  0  0  0  1  0  0 83  3  0  0]\n",
      " [ 1  0  0  8  1  0  0  0  0  0  1  1  0  0  0  0  4 84  0  0]\n",
      " [ 1  0  0  1  4  0  0  2  0  0  0  2  0  0  0  0  0  1 89  0]\n",
      " [ 0  0  5  0  0  1  0  2  0  0  0  1  0  0  0  0  0  0  0 91]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report :-\\n\\n', classification_report(Y_test, ypred_cls))\n",
    "print('Confusion Matrix :-\\n\\n', confusion_matrix(Y_test, ypred_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison my implementation with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we are getting same accuracy score as of sklearn's implementation of MultinomialNB i.e. 84.1%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codingninja",
   "language": "python",
   "name": "codingninja"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
